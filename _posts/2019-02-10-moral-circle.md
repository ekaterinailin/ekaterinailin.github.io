---
layout: post
title: "Moral orbits"
excerpt: "Not only can astronomy and moral philosophy be thought together, they likely ought to."
categories: [EFFECTIVE ALTRUISM]
author: ekaterina
comments: true
image:
  feature: https://images.pexels.com/photos/1567069/pexels-photo-1567069.jpeg?auto=compress&cs=tinysrgb&h=650&w=940
  credit: yuting gao
  creditlink: https://www.pexels.com/photo/silhouette-of-two-person-standing-during-nighttime-1567069/
---

It is of no concern where, spatio-temporally, a moral patient is situated. If a sentient being is or will be suffering, I am morally obliged to help to stop or prevent that suffering. The more this patient is, or will be, suffering, the worse it is not to act.

If you are familiar with the statements above, read on. You need not agree, or disagree. If you are, however, greatly uncertain about what all of this just meant, this post will be difficult to follow. I may elaborate on details about these statements later, but for now, I just want to jot one idea down:

Consider the expanding moral circle. It is common sense in western societies to regard all humans as having equal moral value. But this was a long and hard road. It was paved with slavery, antisemitism, racism, and misogyny. We are far from reaching high moral ground, but things have generally improved. This somewhat encouraging development can be put on a trajectory of a growing number of potential moral patients. From closest to you to complete strangers, it looks approximately like this:

myself &rarr; family and friends &rarr; local community &rarr; extended community &rarr; nation &rarr; supranational level &rarr; world.

For most people, the moral circle encloses, beyond themselves, their relatives and dear friends. Others, many fewer, are equally concerned about everyone and everywhere, even if they do not yet exist. You can add another axis to this range if you include non-human sentient beings, say elephants or chicken, and a time axis. Attitudes probably exist everywhere on this range, from a very narrow towards a extraordinarily wide moral circle.

On the one hand, there is no obvious way to go further left on the line above. You might speculate about someone who is mostly morally concerned only about some part of theirs. I, personally, will set aside the though of an even narrower moral circle than pure self-interest. On the other hand, the obvious question for the right edge of the line is this. Is there, outside of Earth's closest vicinity (hey there, ISS), a moral patient whom I should care about?

If the answer is yes, and the statements at the beginning of this post are convincing enough, I ought to include these sentient beings into my moral circle. However, we are highly uncertain about the existence of extraterrestrial sentient beings (ESBs). Anything goes, from millions of civilizations in the Milky Way to Our Lonely Human Race (OLHR) being the only one out there.

The question is this: Am I morally obliged to help find out if there is anyone out there other than OLHR, who deserves my attention as a moral patient? I believe that the answer is yes. Let me explain.

The answer is yes because, while nobody knows everything, we want, all considered, to make _informed_ moral decisions. We can call

**striving to learn all the morally relevant facts**

a secondary moral value. This can justify a very general exploration directive but I will talk about this moral value in a qualified sense:

**_Our Qualified Obligation to Learn_ If we have reason to believe that there is a morally relevant fact within our reach, we ought to strive to learn this morally relevant fact or to disprove its existence.**
{: .notice}

The crucial word is _relevant_, and there's the rub. As mentioned before, the uncertainty about the existence of ESBs is very high. If we suspect moral patients in a gold mine in China, we have a strong prior that we will find great numbers of people working hard under awful conditions, dying prematurely, and painfully. Most, I believe, would agree, that in this case, we ought to go to these mines. _At the very least, we should check_ if there is any truth behind this suspicion. And even in cases where there is often less compelling evidence that suffering might be underway, we agree that we should try to add transparency to the process. Think food security, or physician-patient relations. In other situations, we have little moral obligation to search for suffering. No-one should be morally concerned with what happens inside your dishwasher while it cleans your dinner plates.

But what if we are _very_ uncertain? To some degree, we can turn to the expected amount of suffering we may find if we search. If the chances are very high that something that is just a tiny bit morally concerning is true, the overall suffering may still be considerable and worth relieving. If the risk of a grand catastrophe happening is very low but the impact would be disastrous, we still ought to prevent it or be prepared to keep the effects at bay. In our case, however, we are very uncertain about a probability distribution. So talking expected values is of little use. But must try something different.

The number of ESBs in our Galaxy (to keep arguments about causality limits out of this debate) ranges from zero to maybe 10^16 (a million populations with 10 billion individuals each). If we are clueless our probability distribution will be flat. Any number will be very unlikely. After finding any evidence, in favor or against some range of population size, we can update our distribution, and re-assess the case for further exploration. This is what we should do, according to _Our Qualified Obligation to Learn_.

Now there is a number of objections. To name a few:

Even if we learn something morally revelant, we will be unable to do anything about it, at least for now. Another objection is that we should rather use our scarce resources on problems much more salient to us, and these give us more than enough causes to focus on for the upcoming centuries and beyond. Yet another objection appeals to the [Fermi paradox](https://en.wikipedia.org/wiki/Fermi_paradox). And you could think of marginal returns: Why bother about something that would happen sooner or later anyway (but certainly on time). Then the costs of making it happen earlier may simply be too high for the benefits.

I will attempt to complement this list of objections, and I will try to answer them with appropriate rigor in later posts. The bottom line in this post shall, for the moment, be just this:

It is well possible that we have a moral reason to dedicate considerable resources to research that will help us constrain the population size of extraterrestrial sentient beings.
{: .notice}

_Thoughts on this? Tag @astroilin and #moralorbit on twitter, or let me know: ekaterina dot ilin at posteo dot de_
